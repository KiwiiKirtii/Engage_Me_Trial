{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: TEST\\Actor_21\\01-01-01-01-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-01-01-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-01-01-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-01-01-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-02-01-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-02-01-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-02-01-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-02-01-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-02-02-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-02-02-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-02-02-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-02-02-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-03-01-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-03-01-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-03-01-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-03-01-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-03-02-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-03-02-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-03-02-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-03-02-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-04-01-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-04-01-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-04-01-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-04-01-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-04-02-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-04-02-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-04-02-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-04-02-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-05-01-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-05-01-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-05-01-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-05-01-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-05-02-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-05-02-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-05-02-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-05-02-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-06-01-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-06-01-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-06-01-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-06-01-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-06-02-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-06-02-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-06-02-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-06-02-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-07-01-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-07-01-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-07-01-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-07-01-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-07-02-01-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-07-02-01-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-07-02-02-01-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-07-02-02-02-21.mp4\n",
      "Processing video: TEST\\Actor_21\\01-01-08-01-01-01-21.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m             video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(actor_path, video_file)\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m             \u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_faces\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Repeat for test folder (optional)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m actor_folder \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(test_folder):\n",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m, in \u001b[0;36mdetect_faces\u001b[1;34m(video_path, save_faces, output_folder)\u001b[0m\n\u001b[0;32m     27\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaleFactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminNeighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Draw rectangles around detected faces\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m faces:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "\n",
    "# # Define the paths for train and test folders\n",
    "# train_folder = 'TEST'  # Update with the path to your train folder\n",
    "# test_folder = 'TRAIN'    # Update with the path to your test folder\n",
    "\n",
    "# # Haar Cascade for face detection (pre-trained XML file provided by OpenCV)\n",
    "# haar_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "# face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n",
    "\n",
    "# # Function for face detection\n",
    "# def detect_faces(video_path, save_faces=False, output_folder=\"output_faces\"):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     frame_count = 0\n",
    "\n",
    "#     # Create the output folder if saving faces\n",
    "#     if save_faces and not os.path.exists(output_folder):\n",
    "#         os.makedirs(output_folder)\n",
    "\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         # Convert to grayscale for face detection\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#         # Detect faces\n",
    "#         faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "#         # Draw rectangles around detected faces\n",
    "#         for (x, y, w, h) in faces:\n",
    "#             cv2.rectangle(frame, (x, y), (xq + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "#             # Optionally save detected faces as individual images\n",
    "#             if save_faces:\n",
    "#                 face = frame[y:y+h, x:x+w]\n",
    "#                 output_path = os.path.join(output_folder, f\"face_{frame_count}.jpg\")\n",
    "#                 cv2.imwrite(output_path, face)\n",
    "\n",
    "#         # Display the frame with detected faces (optional)\n",
    "#         cv2.imshow('Face Detection', frame)\n",
    "\n",
    "#         # Break loop on 'q' key press\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#         frame_count += 1\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# # Process all videos in the train folder\n",
    "# for actor_folder in os.listdir(train_folder):\n",
    "#     actor_path = os.path.join(train_folder, actor_folder)\n",
    "#     if os.path.isdir(actor_path):\n",
    "#         for video_file in os.listdir(actor_path):\n",
    "#             video_path = os.path.join(actor_path, video_file)\n",
    "#             print(f\"Processing video: {video_path}\")\n",
    "#             detect_faces(video_path, save_faces=True, output_folder=\"train_faces\")\n",
    "\n",
    "# # Repeat for test folder (optional)\n",
    "# for actor_folder in os.listdir(test_folder):\n",
    "#     actor_path = os.path.join(test_folder, actor_folder)\n",
    "#     if os.path.isdir(actor_path):\n",
    "#         for video_file in os.listdir(actor_path):\n",
    "#             video_path = os.path.join(actor_path, video_file)\n",
    "#             print(f\"Processing video: {video_path}\")\n",
    "#             detect_faces(video_path, save_faces=True, output_folder=\"test_faces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, classification_report\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import AdditiveChi2Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder='TRAIN'\n",
    "test_folder='TEST'\n",
    "\n",
    "emotion_mapping={\"01\": \"Neutral\",\"02\": \"Calm\",\"03\": \"Happy\",\"04\": \"Sad\",\"05\": \"Angry\",\"06\": \"Fearful\",\"07\": \"Disgust\",\"08\": \"Surprised\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection(one_frame):\n",
    "    haar_cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n",
    "    gray = cv2.cvtColor(one_frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    return faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_processing(video_path):\n",
    "    video_capture=cv2.VideoCapture(video_path)\n",
    "    LBP_features=[]\n",
    "    while video_capture.isOpened():\n",
    "        ret, frame=video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        faces=face_detection(frame)  #detecting faces-> faces are the detected faces which can now be used to extract lbp features\n",
    "        # Extract features for each face\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            face_resized = cv2.resize(face, (128, 128))\n",
    "           \n",
    "            gray = cv2.cvtColor(face_resized, cv2.COLOR_BGR2GRAY)\n",
    "            lbp = local_binary_pattern(gray, P=8, R=1, method=\"uniform\")\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 27), density=True) #hist is the lbp feature extracted\n",
    "            LBP_features.append(hist)\n",
    "    video_capture.release()\n",
    "    return np.mean(LBP_features, axis=0) if LBP_features else np.zeros((26,))\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: TRAIN\\Actor_01\\01-01-01-01-01-01-01.mp4\n",
      "Processing video: TRAIN\\Actor_01\\01-01-01-01-01-02-01.mp4\n",
      "Processing video: TRAIN\\Actor_01\\01-01-01-01-02-01-01.mp4\n"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "max_vids=3\n",
    "current=0\n",
    "for actor_folder in os.listdir(train_folder):\n",
    "    if current >= max_vids:  # Stop once the total videos processed reaches the limit\n",
    "        break\n",
    "    actor_path=os.path.join(train_folder, actor_folder)\n",
    "    if  os.path.isdir(actor_path):\n",
    "        for video in os.listdir(actor_path):\n",
    "                if current >= max_vids:  # Stop once the total videos processed reaches the limit\n",
    "                  break\n",
    "                video_path_train=os.path.join(actor_path, video)\n",
    "                emotion_number=video.split(\"-\")[2]\n",
    "                emotion_value=emotion_mapping[emotion_number]\n",
    "                print(f\"Processing video: {video_path_train}\")\n",
    "                features_after_processing=video_processing(video_path_train)\n",
    "                x.append(features_after_processing)\n",
    "                y.append(emotion_value)\n",
    "                current+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(x)\n",
    "Y_train=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03758894 0.05170425 0.05090768 0.11993097 0.22922002 0.16487215\n",
      "  0.06125376 0.06786673 0.12077985 0.09587564 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.03753724 0.05260861 0.05095757 0.12218314 0.23291324 0.16256621\n",
      "  0.06137609 0.06726999 0.11663263 0.09595528 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.04227786 0.05347183 0.05225176 0.11857495 0.22148714 0.15978438\n",
      "  0.06236535 0.06952282 0.11800361 0.10226031 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
